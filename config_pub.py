from schnet import schnet
from physnet import physnet as physnet
from torchmdnet import torchmdnet
from mpnn import mpnn
from crystals import cgcnn, cgcnn_original
from crystals import physnet as cphysnet
from crystals import schnet as cschnet
from crystals import torchmdnet as ctorchmdnet
from crystals import transformer_head as graph_transformer

import torch

__all__ = ["BACKBONES", "BACKBONE_KWARGS"]

BACKBONES = {
			"schnet": schnet.SchNet,
			 "physnet": physnet.Physnet,
			 "torchmdnet": torchmdnet.TorchMD_Net,
			"cgcnn": cgcnn.CrystalGraphConvNet,
			"cschnet": cschnet.SchNet,
			 "cphysnet": cphysnet.Physnet,
			 "mpnn": mpnn.MPNN,
			}

#WIP for TorchMDNet Configs!
BACKBONE_KWARGS = {
			"schnet": dict(hidden_channels=128, 
				 num_filters=128,
				 num_interactions=6,
				 num_gaussians=50,
				 cutoff=10.0,
				 max_num_neighbors=32,
				 readout='add',
				 dipole=False,
				 mean=None,
				 std=None,
				 atomref=None),
			 "physnet": dict(dfilter=128, 
					 filter=128, 
					 cutoff=10, 
					 num_residuals=3,
					 num_residuals_atomic=2, 
					 num_interactions=5, 
				 	 num_outer_residuals=1,
					 activation_fn=torch.nn.ReLU(), 
					 dmodel=64, 
					 token_embedding_necessary=True, 
					 max_num_neighbors=32, readout="sum"),
			"cschnet": dict(hidden_channels=128, 
			 num_filters=128,
			 num_interactions=6,
			 num_gaussians=50,
			 cutoff=10.0,
			 max_num_neighbors=32,
			 readout='add',
			 dipole=False,
			 mean=None,
			 std=None,
			 atomref=None),
			 "cphysnet": dict(dfilter=64, 
					 filter=64, 
					 cutoff=10, 
					 num_residuals=3,
					 num_residuals_atomic=2, 
					 num_interactions=5, 
					 activation_fn=torch.nn.ReLU(), 
					 dmodel=64, 
					 token_embedding_necessary=True, 
					 max_num_neighbors=32, readout="mean"),
			 "torchmdnet": dict(activation= "silu",
					aggr= "add",
					atom_filter= -1,
					attn_activation= "silu",
					batch_size= 128,
					coord_files= "null",
					cutoff_lower= 0.0,
					cutoff_upper= 5.0,
					dataset= "QM9",
					dataset_arg= "energy_U0",
					dataset_root= "~/data",
					derivative= True,
					distance_influence= "both",
					distributed_backend= "ddp",
					early_stopping_patience= 150,
					ema_alpha_dy= 1.0,
					ema_alpha_y= 1.0,
					embed_files= "null",
					embedding_dimension= 256,
					energy_files= "null",
					energy_weight= 1.0,
					force_files= "null",
					force_weight= 1.0,
					inference_batch_size= 128,
					load_model= "null",
					log_dir= "logs/",
					lr= 0.0004,
					lr_factor= 0.8,
					lr_min= 1.0e-07,
					lr_patience= 15,
					lr_warmup_steps= 10000,
					max_num_neighbors= 64,
					max_z= 100,
					model= "equivariant-transformer",
					neighbor_embedding= True,
					ngpus= -1,
					num_epochs= 3000,
					num_heads= 8,
					num_layers= 8,
					num_nodes= 1,
					num_rbf= 64,
					num_workers= 6,
					output_model= "Scalar",
					precision= 32,
					prior_model= "Atomref",
					rbf_type= "expnorm",
					redirect= False,
					reduce_op= "add",
					save_interval= 10,
					splits= "null",
					standardize= False,
					test_interval= 10,
					test_size= "null",
					train_size= 110000,
					trainable_rbf= False,
					val_size= 10000,
					weight_decay= 0.0
				       ),
			"cgcnn": dict(orig_atom_fea_len=92, nbr_fea_len=41,
			 atom_fea_len=64, n_conv=3, h_fea_len=128, n_h=1,
			 classification=False, learnable=False, explain=False),
			 "mpnn": dict(num_features = 64, num_edge_features = 64, dim1=64, dim2=64, dim3=64, pre_fc_count=1, gc_count=3, post_fc_count=1, pool="global_mean_pool", pool_order="early", batch_norm="False", batch_track_stats="True", act="relu", dropout_rate=0.0, cutoff = 10.,max_num_neighbors=32, num_gaussians=64, heads=8, nlp="gru", explain=False),
			 "mpnn_transformer": dict(num_features = 64, num_edge_features = 64, dim1=64, dim2=64, dim3=64, pre_fc_count=1, gc_count=3, post_fc_count=1, pool="global_mean_pool", pool_order="early", batch_norm="False", batch_track_stats="True", act="relu", dropout_rate=0.0, cutoff = 10.,max_num_neighbors=32, num_gaussians=64, heads=8, nlp="transformer", explain=False),
			 }
